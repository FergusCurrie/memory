{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/07 02:12:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+-------------+--------+----------+---+\n",
      "|   year|students|us_students|undergraduate|graduate|non_degree|opt|\n",
      "+-------+--------+-----------+-------------+--------+----------+---+\n",
      "|1948/49|   25464|  2403400.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1949/50|   26433|  2445000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1950/51|   29813|  2281000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1951/52|   30462|  2102000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1952/53|   33675|  2134000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1953/54|   33833|  2231000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1954/55|   34232|  2447000.0|      19101.0| 12118.0|       NaN|NaN|\n",
      "|1955/56|   36494|  2653000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1956/57|   40666|  2918000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1957/58|   43391|  3324000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1958/59|   47245|        NaN|          NaN|     NaN|       NaN|NaN|\n",
      "|1959/60|   48486|  3640000.0|      25164.0| 18910.0|       NaN|NaN|\n",
      "|1960/61|   53107|        NaN|          NaN|     NaN|       NaN|NaN|\n",
      "|1961/62|   58086|  4146000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1962/63|   64705|        NaN|          NaN|     NaN|       NaN|NaN|\n",
      "|1963/64|   74814|  4780000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1964/65|   82045|  5280000.0|      38130.0| 35096.0|       NaN|NaN|\n",
      "|1965/66|   82709|  5921000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1966/67|  100262|  6390000.0|          NaN|     NaN|       NaN|NaN|\n",
      "|1967/68|  110315|  6912000.0|          NaN|     NaN|       NaN|NaN|\n",
      "+-------+--------+-----------+-------------+--------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`opt`, `year`, `graduate`, `students`, `non_degree`].;\n'Project ['Name]\n+- LogicalRDD [year#0, students#1L, us_students#2, undergraduate#3, graduate#4, non_degree#5, opt#6], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform a simple transformation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwhere(df\u001b[38;5;241m.\u001b[39mAge \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Stop the SparkSession\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/memory-7yvjw6ke-py3.12/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/memory-7yvjw6ke-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/memory-7yvjw6ke-py3.12/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`opt`, `year`, `graduate`, `students`, `non_degree`].;\n'Project ['Name]\n+- LogicalRDD [year#0, students#1L, us_students#2, undergraduate#3, graduate#4, non_degree#5, opt#6], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Create a local SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "x = pd.read_csv(\"backend/code_execution/data/\" + 'academic.csv')\n",
    "\n",
    "\n",
    "# Create a simple DataFrame\n",
    "#data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "df = spark.createDataFrame(x)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Perform a simple transformation\n",
    "result = df.select(\"Name\").where(df.Age > 1)\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['academic.csv',\n",
       " 'us_videos.csv',\n",
       " 'historical_temperatures.csv',\n",
       " 'student_performance.csv',\n",
       " 'pytest_expected_output.csv',\n",
       " 'status.csv',\n",
       " 'venture_funding_deals.parquet',\n",
       " 'venture_funding_deals_partitioned',\n",
       " 'temperatures.csv',\n",
       " 'customer_shopping_data.csv',\n",
       " 'venture_funding_deals_delta',\n",
       " 'README.md',\n",
       " 'world_population.jsonl',\n",
       " 'pokemon.csv',\n",
       " 'ga_20170801.json',\n",
       " 'titanic_dataset.csv',\n",
       " 'contoso_sales.csv',\n",
       " 'my_iceberg_catalog',\n",
       " 'google_store_reviews.csv',\n",
       " 'toronto_weather.csv',\n",
       " 'academic_detail.csv',\n",
       " 'world_population.json',\n",
       " 'customer_shopping_data_no_header.csv',\n",
       " 'world_population.avro',\n",
       " 'covid_19_deaths.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.listdir(\"backend/code_execution/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark.sql import SparkSession\n",
      "spark = SparkSession.builder.appName(\"LocalSparkExample\").master(\"local[*]\").getOrCreate()\n",
      "academic = spark.createDataFrame(academic)\n",
      "result = academic.select(\"students\").where(academic.students > 1)\n",
      "result = result.toPandas()\n",
      "spark.stop()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    students\n",
       " 0      25464\n",
       " 1      26433\n",
       " 2      29813\n",
       " 3      30462\n",
       " 4      33675\n",
       " ..       ...\n",
       " 70   1095299\n",
       " 71   1075496\n",
       " 72    914095\n",
       " 73    948519\n",
       " 74   1057188\n",
       " \n",
       " [75 rows x 1 columns],\n",
       " None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "\n",
    "\n",
    "def get_pyspark_code(code, dfs):\n",
    "    session = 'from pyspark.sql import SparkSession\\nspark = SparkSession.builder.appName(\"LocalSparkExample\").master(\"local[*]\").getOrCreate()\\n'\n",
    "    for df_name in dfs:\n",
    "        session += f\"{df_name} = spark.createDataFrame({df_name})\\n\"\n",
    "\n",
    "    session += code\n",
    "    session += \"\\nresult = result.toPandas()\\nspark.stop()\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def run_code(code, datasets, preprocessing_code):\n",
    "    \"\"\"Execute code and return result\n",
    "\n",
    "    Args:\n",
    "        code (string): code to execute\n",
    "        datasets (list[str]): list of csv datasets to load in\n",
    "        preprocessing_code (string): any initial code to execute\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, str : Resulting dataframe and error (if error raised)\n",
    "    \"\"\"\n",
    "\n",
    "    # Loading datasets in as a dictionary\n",
    "    dfs = {}\n",
    "    for dataset in datasets:\n",
    "        x = pd.read_csv(\"backend/code_execution/data/\" + dataset)\n",
    "        dfs[dataset.replace(\".csv\", \"\")] = x\n",
    "\n",
    "\n",
    "    pyspark_code = get_pyspark_code(code, dfs)    \n",
    "\n",
    "    output = io.StringIO()\n",
    "    error = io.StringIO()\n",
    "    \n",
    "    print(pyspark_code)\n",
    "\n",
    "    try:\n",
    "        with redirect_stdout(output), redirect_stderr(error):\n",
    "            local_vars = dfs\n",
    "            exec(pyspark_code, {}, local_vars)\n",
    "                \n",
    "\n",
    "        if \"result\" in local_vars:\n",
    "            result = local_vars[\"result\"]\n",
    "            # Convert result to DataFrame if it's a Series\n",
    "            if isinstance(result, pl.Series):\n",
    "                result = result.to_frame()\n",
    "            result = result.to_dict()\n",
    "            return pd.DataFrame(result), None\n",
    "        return pd.DataFrame(), None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "run_code('result = academic.select(\"students\").where(academic.students > 1)', ['academic.csv'], \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memory-7yvjw6ke-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
